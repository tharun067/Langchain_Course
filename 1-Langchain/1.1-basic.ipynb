{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc04ef68",
   "metadata": {},
   "source": [
    "#### Getting started with Langchain and Openai API / Google API\n",
    "\n",
    "*  Get setup with Langchain, Langsmith and Langserve.\n",
    "*  Use the most basic and common components of Langchian: Prompt templates, models and output parsers.\n",
    "*  Build a simple application with Langchian\n",
    "*  Trace your application with LangSmith\n",
    "*  Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2f5cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]= os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18bd803e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='models/gemini-2.5-flash' google_api_key=SecretStr('**********') client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000013A5CD63380> default_metadata=() model_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49aced30",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input and get response from llm\n",
    "results = llm.invoke(\"What is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "128716ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Generative AI** refers to a category of artificial intelligence models that are designed to **create new and original content** rather than just analyzing or classifying existing data. Unlike traditional AI that might identify a cat in a picture, generative AI can *generate* a brand new picture of a cat that has never existed before.\n",
      "\n",
      "Here's a breakdown of what that means:\n",
      "\n",
      "1.  **Creation, Not Just Analysis:**\n",
      "    *   **Discriminative AI** (like image recognition or spam filters) learns to distinguish between different types of data. It answers questions like \"Is this X or Y?\" or \"What is this?\"\n",
      "    *   **Generative AI** learns to produce new instances of data that resemble the data it was trained on. It answers questions like \"Create an X\" or \"What would an X look like?\"\n",
      "\n",
      "2.  **How It Works (Simplified):**\n",
      "    *   **Training Data:** Generative AI models are trained on vast datasets (e.g., millions of images, billions of text snippets, hours of audio).\n",
      "    *   **Learning Patterns:** During training, the model identifies underlying patterns, structures, and relationships within this data. It learns the \"rules\" of what makes a realistic image, a coherent sentence, or a melodious tune.\n",
      "    *   **Generation:** When given a prompt or input, the model uses its learned understanding to synthesize new data that adheres to those patterns, creating something novel but consistent with its training.\n",
      "\n",
      "3.  **Key Characteristics:**\n",
      "    *   **Novelty:** The output is genuinely new and not just a copy of something in its training data.\n",
      "    *   **Realism/Plausibility:** The generated content often looks, sounds, or reads very authentically, making it hard to distinguish from human-created content.\n",
      "    *   **Versatility:** Can operate across various modalities (text, images, audio, video, code, 3D models).\n",
      "    *   **Contextual Understanding:** Many generative models (especially Large Language Models) demonstrate a remarkable ability to understand and respond to complex prompts, maintaining context over extended interactions.\n",
      "\n",
      "4.  **Common Applications and Examples:**\n",
      "\n",
      "    *   **Text Generation (Large Language Models - LLMs):**\n",
      "        *   Writing articles, stories, poems, scripts, emails, marketing copy.\n",
      "        *   Summarizing long texts.\n",
      "        *   Translating languages.\n",
      "        *   Answering questions and engaging in conversations (chatbots like ChatGPT).\n",
      "        *   Generating code snippets or debugging.\n",
      "    *   **Image Generation (Text-to-Image models like DALL-E, Midjourney, Stable Diffusion):**\n",
      "        *   Creating realistic or artistic images from text descriptions.\n",
      "        *   Generating variations of existing images.\n",
      "        *   Image editing, style transfer, and inpainting (filling in missing parts).\n",
      "        *   Creating concept art or product designs.\n",
      "    *   **Audio Generation:**\n",
      "        *   Composing music in various styles.\n",
      "        *   Generating realistic speech from text (text-to-speech).\n",
      "        *   Creating sound effects.\n",
      "        *   Voice cloning.\n",
      "    *   **Video Generation:**\n",
      "        *   Generating short video clips from text or images.\n",
      "        *   Manipulating existing video footage (e.g., deepfakes, style transfer).\n",
      "    *   **Code Generation:**\n",
      "        *   Assisting developers by writing code, suggesting functions, or debugging.\n",
      "    *   **3D Model Generation:**\n",
      "        *   Creating 3D objects for games, simulations, or design.\n",
      "\n",
      "5.  **Underlying Technologies:**\n",
      "    *   **Generative Adversarial Networks (GANs):** Involve two neural networks, a \"generator\" that creates content and a \"discriminator\" that tries to tell if it's real or fake. They train against each other, improving until the generator can fool the discriminator.\n",
      "    *   **Variational Autoencoders (VAEs):** Learn a compressed representation of data and then decode it back into new content.\n",
      "    *   **Transformer Models:** Particularly powerful for sequential data like text, forming the backbone of most LLMs.\n",
      "    *   **Diffusion Models:** Currently state-of-the-art for image generation, these models learn to gradually remove noise from an image until a coherent output is produced.\n",
      "\n",
      "Generative AI is a rapidly evolving field that is transforming industries by automating creative tasks, enabling new forms of content creation, and pushing the boundaries of what machines can produce. It also comes with significant ethical considerations regarding misinformation, copyright, bias, and job displacement.\n"
     ]
    }
   ],
   "source": [
    "print(results.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "900a78aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the question.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are an expert AI Engineer. Provide me answers based on the question.\"),\n",
    "    (\"user\",\"{input}\")\n",
    "])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "056e2f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a **developer platform for building, debugging, testing, and monitoring robust Large Language Model (LLM) applications**. It's developed by the same team behind the popular LangChain framework and is designed to address the unique challenges of developing and deploying LLM-powered systems.\n",
      "\n",
      "Think of it as an \"observability platform\" specifically tailored for the non-deterministic and complex nature of LLM applications.\n",
      "\n",
      "## Why Was LangSmith Created? (The Problem It Solves)\n",
      "\n",
      "Developing LLM applications is notoriously difficult due to several factors:\n",
      "\n",
      "1.  **Non-Determinism:** LLMs don't always give the same output for the same input, making traditional debugging challenging.\n",
      "2.  **Complexity:** LLM applications often involve multiple steps (e.g., prompt engineering, tool usage, retrieval augmented generation (RAG), external API calls), making it hard to track the flow and identify where things go wrong.\n",
      "3.  **Lack of Visibility:** It's tough to understand what's happening \"under the hood\" â€“ what the LLM is thinking, which tools it's using, or why it made a certain decision.\n",
      "4.  **Evaluation Challenges:** Traditional unit tests don't fit well for LLM outputs. How do you objectively measure \"goodness\" or \"relevance\"?\n",
      "5.  **Production Monitoring:** Once deployed, it's hard to monitor performance, cost, latency, and detect regressions or unexpected behavior.\n",
      "\n",
      "LangSmith aims to solve these problems by providing a comprehensive suite of tools.\n",
      "\n",
      "## Key Features of LangSmith\n",
      "\n",
      "1.  **Tracing & Debugging:**\n",
      "    *   **Visualize Call Chains:** See a complete, step-by-step breakdown of every interaction in your LLM application, including LLM calls, tool invocations, retriever steps, and custom code.\n",
      "    *   **Input/Output Inspection:** Examine the exact inputs and outputs at each stage of your chain.\n",
      "    *   **Error Identification:** Quickly pinpoint where errors occurred in complex multi-step processes.\n",
      "    *   **Latency Analysis:** Understand which parts of your application are slow and optimize them.\n",
      "    *   **\"Chain of Thought\" Visibility:** Get insights into how the LLM is reasoning.\n",
      "\n",
      "2.  **Testing & Evaluation:**\n",
      "    *   **Dataset Creation:** Easily create and manage datasets of inputs and desired outputs (or even complex evaluation criteria) from your existing traces or by manually adding examples.\n",
      "    *   **Run Test Cases:** Execute your application against these datasets to check its performance and identify regressions.\n",
      "    *   **Custom Evaluators:** Define your own evaluation metrics, including using LLMs to evaluate other LLMs' outputs, or traditional rule-based metrics.\n",
      "    *   **A/B Testing & Comparison:** Compare different versions of your application, prompt templates, or models side-by-side to determine which performs better.\n",
      "\n",
      "3.  **Monitoring & Observability (Production):**\n",
      "    *   **Performance Metrics:** Track key metrics like latency, cost, token usage, and error rates in live applications.\n",
      "    *   **User Feedback Integration:** Capture user feedback directly from your application to understand real-world performance.\n",
      "    *   **Regression Detection:** Identify when new deployments or changes negatively impact performance.\n",
      "    *   **Alerting:** Set up alerts for critical issues or performance degradations.\n",
      "\n",
      "4.  **Dataset Management:**\n",
      "    *   **Curate Examples:** Collect and curate high-quality examples from your application runs, which can be used for fine-tuning, RAG, or further testing.\n",
      "    *   **Version Control:** Manage different versions of your datasets and experiments.\n",
      "\n",
      "5.  **Collaboration:**\n",
      "    *   Share traces, datasets, and evaluation results with your team members, fostering collaborative development and debugging.\n",
      "\n",
      "## How it Works\n",
      "\n",
      "LangSmith integrates with your LLM application, typically through an SDK. When your application runs, LangSmith automatically captures detailed information about each step (inputs, outputs, LLM calls, tool usage, etc.) and sends it to the LangSmith platform. This data is then visualized in a user-friendly web interface, allowing developers to inspect, analyze, and manage their LLM applications.\n",
      "\n",
      "While deeply integrated with LangChain, LangSmith can also be used with other LLM frameworks and custom code via its SDK.\n",
      "\n",
      "## Benefits of Using LangSmith\n",
      "\n",
      "*   **Faster Iteration:** Debug and identify issues much more quickly.\n",
      "*   **Higher Quality Applications:** Rigorous testing and evaluation lead to more reliable and accurate LLM apps.\n",
      "*   **Cost Efficiency:** Monitor token usage and latency to optimize costs.\n",
      "*   **Improved Reliability:** Proactive monitoring helps prevent production issues and detect regressions.\n",
      "*   **Better Understanding:** Gain deep insights into the internal workings of your LLM applications.\n",
      "*   **Data-Driven Improvement:** Collect valuable data to inform prompt engineering, fine-tuning, and overall system improvement.\n",
      "\n",
      "In essence, LangSmith is an indispensable tool for anyone serious about building, deploying, and maintaining production-ready LLM applications, helping to bridge the gap between experimental prototypes and robust, reliable AI systems.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"input\":\"Can you tell me about LangSmith?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46cb27ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da6c0159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a **developer platform for building, debugging, testing, and monitoring Large Language Model (LLM) applications**. Developed by the same team behind LangChain, it's designed to be the \"control plane\" that brings observability and reliability to the inherently complex and non-deterministic world of LLM-powered applications.\n",
      "\n",
      "Think of it as an indispensable tool for anyone moving beyond simple LLM prompts to building robust, production-ready applications like chatbots, agents, RAG systems, or custom LLM workflows.\n",
      "\n",
      "Here's a breakdown of what LangSmith is and why it's so important:\n",
      "\n",
      "## What Problem Does LangSmith Solve?\n",
      "\n",
      "Developing with LLMs presents unique challenges:\n",
      "\n",
      "1.  **Non-Determinism:** LLMs don't always give the same output for the same input, making traditional debugging difficult.\n",
      "2.  **Black Box Nature:** It's hard to understand *why* an LLM responded a certain way, especially in multi-step chains or agentic workflows.\n",
      "3.  **Complexity:** LLM applications often involve many components: prompts, LLMs, retrievers, tools, parsers, custom code, all interacting.\n",
      "4.  **Evaluation:** How do you know if your application is getting better or worse? How do you measure performance and quality?\n",
      "5.  **Production Monitoring:** Once deployed, how do you track performance, cost, latency, and user satisfaction? How do you identify regressions or issues?\n",
      "\n",
      "LangSmith addresses these challenges by providing visibility, testing infrastructure, and monitoring capabilities.\n",
      "\n",
      "## Key Features of LangSmith\n",
      "\n",
      "1.  **Tracing & Debugging:**\n",
      "    *   **Visual Execution Traces:** LangSmith automatically logs and visualizes every step of an LLM call or LangChain run. You see a hierarchical view of chains, agents, tools, LLM calls, retrievers, and custom functions.\n",
      "    *   **Detailed Inputs/Outputs:** For each step, you can inspect the exact inputs, outputs, intermediate thoughts, latency, token usage, and even the raw API calls to the LLM.\n",
      "    *   **Error Identification:** Easily pinpoint where errors occurred within a complex chain.\n",
      "    *   **Latency & Cost Analysis:** Understand which parts of your application are slow or expensive.\n",
      "\n",
      "2.  **Testing & Evaluation:**\n",
      "    *   **Dataset Management:** Create and manage datasets of inputs and expected (or actual) outputs to test your application against.\n",
      "    *   **Automated Evaluators:** Run your chains against these datasets and use built-in (e.g., LLM-based evaluators, exact match, similarity) or custom evaluators to automatically score the performance.\n",
      "    *   **Human Feedback:** Collect human feedback on runs, allowing you to refine your models and prompts.\n",
      "    *   **Experimentation & Comparison:** Easily compare the performance of different versions of your chains, prompts, or models over time or across different configurations. This is crucial for A/B testing and iterating on improvements.\n",
      "    *   **Regression Testing:** Ensure that new changes don't degrade performance on existing test cases.\n",
      "\n",
      "3.  **Monitoring & Observability:**\n",
      "    *   **Production Monitoring:** Track key metrics in production, such as latency, token usage, cost, error rates, and user satisfaction (e.g., thumbs up/down feedback).\n",
      "    *   **Alerting:** Set up alerts for anomalies or performance degradation.\n",
      "    *   **Real-time Insights:** Gain insights into how your application is performing in the wild, allowing for proactive adjustments.\n",
      "    *   **Feedback Integration:** Capture user feedback directly from your application and link it back to specific runs in LangSmith for analysis.\n",
      "\n",
      "4.  **Prompt Management:**\n",
      "    *   Version control for your prompts.\n",
      "    *   Experiment with different prompt templates and see their impact on performance.\n",
      "\n",
      "## How it Works (Briefly)\n",
      "\n",
      "LangSmith integrates seamlessly with LangChain (and can also be used with other LLM frameworks/libraries by wrapping your LLM calls). When you enable LangSmith in your environment, your LLM calls and chain executions are automatically logged to the LangSmith platform. You then access a web UI to view your traces, manage datasets, run evaluations, and monitor your applications.\n",
      "\n",
      "## Who is it For?\n",
      "\n",
      "*   **LLM Developers:** For rapid iteration, debugging complex chains, and understanding LLM behavior.\n",
      "*   **MLOps Engineers:** For deploying, monitoring, and maintaining LLM applications in production.\n",
      "*   **Data Scientists/Researchers:** For experimenting with prompts, models, and architectures, and rigorously evaluating results.\n",
      "*   **Teams building any application powered by LLMs:** From simple chatbots to sophisticated autonomous agents.\n",
      "\n",
      "## Relationship to LangChain\n",
      "\n",
      "LangSmith and LangChain are complementary products from the same company.\n",
      "*   **LangChain** provides the framework and tools to *build* LLM applications (chains, agents, retrievers, etc.).\n",
      "*   **LangSmith** provides the platform to *observe, debug, test, and monitor* those applications, especially when they are built with LangChain.\n",
      "\n",
      "While LangSmith is optimized for LangChain, it offers SDKs that allow you to use its tracing and monitoring capabilities even if you're interacting with LLMs directly (e.g., just using the OpenAI Python client).\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "LangSmith is an essential tool for bringing engineering rigor to LLM development. It transforms the often opaque and unpredictable process of building with LLMs into a more transparent, testable, and maintainable workflow, enabling developers to build higher-quality, more reliable, and ultimately, more useful LLM applications.\n"
     ]
    }
   ],
   "source": [
    "## stroutput parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\":\"Can you tell me about LangSmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea1728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
