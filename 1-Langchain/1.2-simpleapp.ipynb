{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c97c68",
   "metadata": {},
   "source": [
    "#### Simple GENAI APPLICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "849a946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]= os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd1b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x2004d2c3ce0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Data Ingestion -- From the website we need to scrape the data\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "590975e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='Evaluate a chatbot - Docs by LangChainOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsEvaluate a chatbotEvaluate a RAG applicationTest a ReAct agent with Pytest/Vitest and LangSmithEvaluate a complex agentRun backtests on a new version of an agentAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KGitHubForumForumSearch...NavigationTutorialsEvaluate a chatbotGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSetupCreate a datasetDefine metricsRun EvaluationsComparing resultsSet up automated testing to run in CI/CDTrack results over timeConclusionReference codeSet up evaluationsTutorialsEvaluate a chatbotCopy pageCopy pageIn this guide we will set up evaluations for a chatbot. These allow you to measure how well your application is performing over a set of data. Being able to get this insight quickly and reliably will allow you to iterate with confidence.\\nAt a high level, in this tutorial we will:\\n\\nCreate an initial golden dataset to measure performance\\nDefine metrics to use to measure performance\\nRun evaluations on a few different prompts or models\\nCompare results manually\\nTrack results over time\\nSet up automated testing to run in CI/CD\\n\\nFor more information on the evaluation workflows LangSmith supports, check out the how-to guides, or see the reference docs for evaluate and its asynchronous aevaluate counterpart.\\nLots to cover, let’s dive in!\\n\\u200bSetup\\nFirst install the required dependencies for this tutorial. We happen to use OpenAI, but LangSmith can be used with any model:\\npipuvCopypip install -U langsmith openai\\n\\nAnd set environment variables to enable LangSmith tracing:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"<Your LangSmith API key>\"\\nexport OPENAI_API_KEY=\"<Your OpenAI API key>\"\\n\\n\\u200bCreate a dataset\\nThe first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:\\n\\nWhat should the schema of each datapoint be?\\nHow many datapoints should I gather?\\nHow should I gather those datapoints?\\n\\nSchema: Each datapoint should consist of, at the very least, the inputs to the application. If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output. Often times you cannot define the perfect output - that’s okay! Evaluation is an iterative process. Sometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent. LangSmith datasets are very flexible and allow you to define arbitrary schemas.\\nHow many: There’s no hard and fast rule for how many you should gather. The main thing is to make sure you have proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value! Don’t worry about getting a large number to start - you can (and should) always add over time!\\nHow to get: This is maybe the trickiest part. Once you know you want to gather a dataset… how do you actually go about it? For most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand. After starting with these datapoints, these datasets are generally living constructs and grow over time. They generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set. There are also methods like synthetically generating data that can be used to augment your dataset. To start, we recommend not worrying about those and just hand labeling ~10-20 examples.\\nOnce you’ve got your dataset, there are a few different ways to upload them to LangSmith. For this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).\\nFor this tutorial, we will create 5 datapoints to evaluate on. We will be evaluating a question-answering application. The input will be a question, and the output will be an answer. Since this is a question-answering application, we can define the expected answer. Let’s show how to create and upload this dataset to LangSmith!\\nCopyfrom langsmith import Client\\n\\nclient = Client()\\n\\n# Define dataset: these are your test cases\\ndataset_name = \"QA Example Dataset\"\\ndataset = client.create_dataset(dataset_name)\\n\\nclient.create_examples(\\n    dataset_id=dataset.id,\\n    examples=[\\n        {\\n            \"inputs\": {\"question\": \"What is LangChain?\"},\\n            \"outputs\": {\"answer\": \"A framework for building LLM applications\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is LangSmith?\"},\\n            \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is OpenAI?\"},\\n            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is Google?\"},\\n            \"outputs\": {\"answer\": \"A technology company known for search\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is Mistral?\"},\\n            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\\n        }\\n    ]\\n)\\n\\nNow, if we go the LangSmith UI and look for QA Example Dataset in the Datasets & Testing page, when we click into it we should see that we have five new examples.\\n\\n\\u200bDefine metrics\\nAfter creating our dataset, we can now define some metrics to evaluate our responses on. Since we have an expected answer, we can compare to that as part of our evaluation. However, we do not expect our application to output those exact answers, but rather something that is similar. This makes our evaluation a little trickier.\\nIn addition to evaluating correctness, let’s also make sure our answers are short and concise. This will be a little easier - we can define a simple Python function to measure the length of the response.\\nLet’s go ahead and define these two metrics.\\nFor the first, we will use an LLM to judge whether the output is correct (with respect to the expected output). This LLM-as-a-judge is relatively common for cases that are too complex to measure with a simple function. We can define our own prompt and LLM to use for evaluation here:\\nCopyimport openai\\nfrom langsmith import wrappers\\n\\nopenai_client = wrappers.wrap_openai(openai.OpenAI())\\n\\neval_instructions = \"You are an expert professor specialized in grading students\\' answers to questions.\"\\n\\ndef correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\\n    user_content = f\"\"\"You are grading the following question:\\n{inputs[\\'question\\']}\\nHere is the real answer:\\n{reference_outputs[\\'answer\\']}\\nYou are grading the following predicted answer:\\n{outputs[\\'response\\']}\\nRespond with CORRECT or INCORRECT:\\nGrade:\"\"\"\\n    response = openai_client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\"role\": \"system\", \"content\": eval_instructions},\\n            {\"role\": \"user\", \"content\": user_content},\\n        ],\\n    ).choices[0].message.content\\n    return response == \"CORRECT\"\\n\\nFor evaluating the length of the response, this is a lot easier! We can just define a simple function that checks whether the actual output is less than 2x the length of the expected result.\\nCopydef concision(outputs: dict, reference_outputs: dict) -> bool:\\n    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))\\n\\n\\u200bRun Evaluations\\nGreat! So now how do we run evaluations? Now that we have a dataset and evaluators, all that we need is our application! We will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM. We will build this using the OpenAI SDK directly:\\nCopydefault_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"\\n\\ndef my_app(question: str, model: str = \"gpt-4o-mini\", instructions: str = default_instructions) -> str:\\n    return openai_client.chat.completions.create(\\n        model=model,\\n        temperature=0,\\n        messages=[\\n            {\"role\": \"system\", \"content\": instructions},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    ).choices[0].message.content\\n\\nBefore running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call, and then also maps the output of the function to the output key we expect.\\nCopydef ls_target(inputs: str) -> dict:\\n    return {\"response\": my_app(inputs[\"question\"])}\\n\\nGreat! Now we’re ready to run an evaluation. Let’s do it!\\nCopyexperiment_results = client.evaluate(\\n    ls_target, # Your AI system\\n    data=dataset_name, # The data to predict and grade over\\n    evaluators=[concision, correctness], # The evaluators to score the results\\n    experiment_prefix=\"openai-4o-mini\", # A prefix for your experiment names to easily identify them\\n)\\n\\nThis will output a URL. If we click on it, we should see results of our evaluation!\\n\\nIf we go back to the dataset page and select the Experiments tab, we can now see a summary of our one run!\\n\\nLet’s now try it out with a different model! Let’s try gpt-4-turbo\\nCopydef ls_target_v2(inputs: str) -> dict:\\n    return {\"response\": my_app(inputs[\"question\"], model=\"gpt-4-turbo\")}\\n\\nexperiment_results = client.evaluate(\\n    ls_target_v2,\\n    data=dataset_name,\\n    evaluators=[concision, correctness],\\n    experiment_prefix=\"openai-4-turbo\",\\n)\\n\\nAnd now let’s use GPT-4 but also update the prompt to be a bit more strict in requiring the answer to be short.\\nCopyinstructions_v3 = \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"\\n\\ndef ls_target_v3(inputs: str) -> dict:\\n    response = my_app(\\n        inputs[\"question\"],\\n        model=\"gpt-4-turbo\",\\n        instructions=instructions_v3\\n    )\\n    return {\"response\": response}\\n\\nexperiment_results = client.evaluate(\\n    ls_target_v3,\\n    data=dataset_name,\\n    evaluators=[concision, correctness],\\n    experiment_prefix=\"strict-openai-4-turbo\",\\n)\\n\\nIf we go back to the Experiments tab on the datasets page, we should see that all three runs now show up!\\n\\n\\u200bComparing results\\nAwesome, we’ve evaluated three different runs. But how can we compare results? The first way we can do this is just by looking at the runs in the Experiments tab. If we do that, we can see a high level view of the metrics for each run:\\n\\nGreat! So we can tell that GPT-4 is better than GPT-3.5 at knowing who companies are, and we can see that the strict prompt helped a lot with the length. But what if we want to explore in more detail?\\nIn order to do that, we can select all the runs we want to compare (in this case all three) and open them up in a comparison view. We immediately see all three tests side by side. Some of the cells are color coded - this is showing a regression of a certain metric compared to a certain baseline. We automatically choose defaults for the baseline and metric, but you can change those yourself. You can also choose which columns and which metrics you see by using the Display control. You can also automatically filter to only see the runs that have improvements/regressions by clicking on the icons at the top.\\n\\nIf we want to see more information, we can also select the Expand button that appears when hovering over a row to open up a side panel with more detailed information:\\n\\n\\u200bSet up automated testing to run in CI/CD\\nNow that we’ve run this in a one-off manner, we can set it to run in an automated fashion. We can do this pretty easily by just including it as a pytest file that we run in CI/CD. As part of this, we can either just log the results OR set up some criteria to determine if it passes or not. For example, if I wanted to ensure that we always got at least 80% of generated responses passing the length check, we could set that up with a test like:\\nCopydef test_length_score() -> None:\\n    \"\"\"Test that the length score is at least 80%.\"\"\"\\n    experiment_results = evaluate(\\n        ls_target, # Your AI system\\n        data=dataset_name, # The data to predict and grade over\\n        evaluators=[concision, correctness], # The evaluators to score the results\\n    )\\n    # This will be cleaned up in the next release:\\n    feedback = client.list_feedback(\\n        run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],\\n        feedback_key=\"concision\"\\n    )\\n    scores = [f.score for f in feedback]\\n    assert sum(scores) / len(scores) >= 0.8, \"Aggregate score should be at least .8\"\\n\\n\\u200bTrack results over time\\nNow that we’ve got these experiments running in an automated fashion, we want to track these results over time. We can do this from the overall Experiments tab in the datasets page. By default, we show evaluation metrics over time (highlighted in red). We also automatically track git metrics, to easily associate it with the branch of your code (highlighted in yellow).\\n\\n\\u200bConclusion\\nThat’s it for this tutorial!\\nWe’ve gone over how to create an initial test set, define some evaluation metrics, run experiments, compare them manually, set up CI/CD, and track results over time. Hopefully this can help you iterate with confidence.\\nThis is just the start. As mentioned earlier, evaluation is an ongoing process. For example - the datapoints you will want to evaluate on will likely continue to change over time. There are many types of evaluators you may wish to explore. For information on this, check out the how-to guides.\\nAdditionally, there are other ways to evaluate data besides in this “offline” manner (e.g. you can evaluate production data). For more information on online evaluation, check out this guide.\\n\\u200bReference code\\nClick to see a consolidated code snippetCopyimport openai\\nfrom langsmith import Client, wrappers\\n\\n# Application code\\nopenai_client = wrappers.wrap_openai(openai.OpenAI())\\n\\ndefault_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"\\n\\ndef my_app(question: str, model: str = \"gpt-4o-mini\", instructions: str = default_instructions) -> str:\\n    return openai_client.chat.completions.create(\\n        model=model,\\n        temperature=0,\\n        messages=[\\n            {\"role\": \"system\", \"content\": instructions},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    ).choices[0].message.content\\n\\nclient = Client()\\n\\n# Define dataset: these are your test cases\\ndataset_name = \"QA Example Dataset\"\\ndataset = client.create_dataset(dataset_name)\\n\\nclient.create_examples(\\n    dataset_id=dataset.id,\\n    examples=[\\n        {\\n            \"inputs\": {\"question\": \"What is LangChain?\"},\\n            \"outputs\": {\"answer\": \"A framework for building LLM applications\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is LangSmith?\"},\\n            \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is OpenAI?\"},\\n            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is Google?\"},\\n            \"outputs\": {\"answer\": \"A technology company known for search\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is Mistral?\"},\\n            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\\n        }\\n    ]\\n)\\n\\n# Define evaluators\\neval_instructions = \"You are an expert professor specialized in grading students\\' answers to questions.\"\\n\\ndef correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\\n    user_content = f\"\"\"You are grading the following question:\\n{inputs[\\'question\\']}\\nHere is the real answer:\\n{reference_outputs[\\'answer\\']}\\nYou are grading the following predicted answer:\\n{outputs[\\'response\\']}\\nRespond with CORRECT or INCORRECT:\\nGrade:\"\"\"\\n    response = openai_client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\"role\": \"system\", \"content\": eval_instructions},\\n            {\"role\": \"user\", \"content\": user_content},\\n        ],\\n    ).choices[0].message.content\\n    return response == \"CORRECT\"\\n\\ndef concision(outputs: dict, reference_outputs: dict) -> bool:\\n    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))\\n\\n# Run evaluations\\ndef ls_target(inputs: str) -> dict:\\n    return {\"response\": my_app(inputs[\"question\"])}\\n\\nexperiment_results_v1 = client.evaluate(\\n    ls_target, # Your AI system\\n    data=dataset_name, # The data to predict and grade over\\n    evaluators=[concision, correctness], # The evaluators to score the results\\n    experiment_prefix=\"openai-4o-mini\", # A prefix for your experiment names to easily identify them\\n)\\n\\ndef ls_target_v2(inputs: str) -> dict:\\n    return {\"response\": my_app(inputs[\"question\"], model=\"gpt-4-turbo\")}\\n\\nexperiment_results_v2 = client.evaluate(\\n    ls_target_v2,\\n    data=dataset_name,\\n    evaluators=[concision, correctness],\\n    experiment_prefix=\"openai-4-turbo\",\\n)\\n\\ninstructions_v3 = \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"\\n\\ndef ls_target_v3(inputs: str) -> dict:\\n    response = my_app(\\n        inputs[\"question\"],\\n        model=\"gpt-4-turbo\",\\n        instructions=instructions_v3\\n    )\\n    return {\"response\": response}\\n\\nexperiment_results_v3 = client.evaluate(\\n    ls_target_v3,\\n    data=dataset_name,\\n    evaluators=[concision, correctness],\\n    experiment_prefix=\"strict-openai-4-turbo\",\\n)\\nWas this page helpful?YesNoDynamic few shot example selectionEvaluate a RAG applicationAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41668fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='Evaluate a chatbot - Docs by LangChainOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsEvaluate a chatbotEvaluate a RAG applicationTest a ReAct agent with Pytest/Vitest and LangSmithEvaluate a complex agentRun backtests on a new version of an agentAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KGitHubForumForumSearch...NavigationTutorialsEvaluate a chatbotGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSetupCreate a datasetDefine metricsRun EvaluationsComparing resultsSet up automated testing to run in CI/CDTrack results over timeConclusionReference codeSet up evaluationsTutorialsEvaluate a chatbotCopy pageCopy pageIn this guide we will set up evaluations for a chatbot. These allow you to measure how well your application is performing over a set of data. Being able to get this insight quickly and reliably will allow you to iterate with confidence.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='At a high level, in this tutorial we will:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='Create an initial golden dataset to measure performance\\nDefine metrics to use to measure performance\\nRun evaluations on a few different prompts or models\\nCompare results manually\\nTrack results over time\\nSet up automated testing to run in CI/CD\\n\\nFor more information on the evaluation workflows LangSmith supports, check out the how-to guides, or see the reference docs for evaluate and its asynchronous aevaluate counterpart.\\nLots to cover, let’s dive in!\\n\\u200bSetup\\nFirst install the required dependencies for this tutorial. We happen to use OpenAI, but LangSmith can be used with any model:\\npipuvCopypip install -U langsmith openai\\n\\nAnd set environment variables to enable LangSmith tracing:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"<Your LangSmith API key>\"\\nexport OPENAI_API_KEY=\"<Your OpenAI API key>\"'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='And set environment variables to enable LangSmith tracing:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"<Your LangSmith API key>\"\\nexport OPENAI_API_KEY=\"<Your OpenAI API key>\"\\n\\n\\u200bCreate a dataset\\nThe first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:\\n\\nWhat should the schema of each datapoint be?\\nHow many datapoints should I gather?\\nHow should I gather those datapoints?'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='Schema: Each datapoint should consist of, at the very least, the inputs to the application. If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output. Often times you cannot define the perfect output - that’s okay! Evaluation is an iterative process. Sometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent. LangSmith datasets are very flexible and allow you to define arbitrary schemas.\\nHow many: There’s no hard and fast rule for how many you should gather. The main thing is to make sure you have proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value! Don’t worry about getting a large number to start - you can (and should) always add over time!'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='How to get: This is maybe the trickiest part. Once you know you want to gather a dataset… how do you actually go about it? For most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand. After starting with these datapoints, these datasets are generally living constructs and grow over time. They generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set. There are also methods like synthetically generating data that can be used to augment your dataset. To start, we recommend not worrying about those and just hand labeling ~10-20 examples.\\nOnce you’ve got your dataset, there are a few different ways to upload them to LangSmith. For this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='For this tutorial, we will create 5 datapoints to evaluate on. We will be evaluating a question-answering application. The input will be a question, and the output will be an answer. Since this is a question-answering application, we can define the expected answer. Let’s show how to create and upload this dataset to LangSmith!\\nCopyfrom langsmith import Client'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='client = Client()\\n\\n# Define dataset: these are your test cases\\ndataset_name = \"QA Example Dataset\"\\ndataset = client.create_dataset(dataset_name)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='client.create_examples(\\n    dataset_id=dataset.id,\\n    examples=[\\n        {\\n            \"inputs\": {\"question\": \"What is LangChain?\"},\\n            \"outputs\": {\"answer\": \"A framework for building LLM applications\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is LangSmith?\"},\\n            \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is OpenAI?\"},\\n            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is Google?\"},\\n            \"outputs\": {\"answer\": \"A technology company known for search\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is Mistral?\"},\\n            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\\n        }\\n    ]\\n)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='Now, if we go the LangSmith UI and look for QA Example Dataset in the Datasets & Testing page, when we click into it we should see that we have five new examples.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='\\u200bDefine metrics\\nAfter creating our dataset, we can now define some metrics to evaluate our responses on. Since we have an expected answer, we can compare to that as part of our evaluation. However, we do not expect our application to output those exact answers, but rather something that is similar. This makes our evaluation a little trickier.\\nIn addition to evaluating correctness, let’s also make sure our answers are short and concise. This will be a little easier - we can define a simple Python function to measure the length of the response.\\nLet’s go ahead and define these two metrics.\\nFor the first, we will use an LLM to judge whether the output is correct (with respect to the expected output). This LLM-as-a-judge is relatively common for cases that are too complex to measure with a simple function. We can define our own prompt and LLM to use for evaluation here:\\nCopyimport openai\\nfrom langsmith import wrappers\\n\\nopenai_client = wrappers.wrap_openai(openai.OpenAI())'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='openai_client = wrappers.wrap_openai(openai.OpenAI())\\n\\neval_instructions = \"You are an expert professor specialized in grading students\\' answers to questions.\"\\n\\ndef correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\\n    user_content = f\"\"\"You are grading the following question:\\n{inputs[\\'question\\']}\\nHere is the real answer:\\n{reference_outputs[\\'answer\\']}\\nYou are grading the following predicted answer:\\n{outputs[\\'response\\']}\\nRespond with CORRECT or INCORRECT:\\nGrade:\"\"\"\\n    response = openai_client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\"role\": \"system\", \"content\": eval_instructions},\\n            {\"role\": \"user\", \"content\": user_content},\\n        ],\\n    ).choices[0].message.content\\n    return response == \"CORRECT\"'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='For evaluating the length of the response, this is a lot easier! We can just define a simple function that checks whether the actual output is less than 2x the length of the expected result.\\nCopydef concision(outputs: dict, reference_outputs: dict) -> bool:\\n    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))\\n\\n\\u200bRun Evaluations\\nGreat! So now how do we run evaluations? Now that we have a dataset and evaluators, all that we need is our application! We will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM. We will build this using the OpenAI SDK directly:\\nCopydefault_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='def my_app(question: str, model: str = \"gpt-4o-mini\", instructions: str = default_instructions) -> str:\\n    return openai_client.chat.completions.create(\\n        model=model,\\n        temperature=0,\\n        messages=[\\n            {\"role\": \"system\", \"content\": instructions},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    ).choices[0].message.content\\n\\nBefore running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call, and then also maps the output of the function to the output key we expect.\\nCopydef ls_target(inputs: str) -> dict:\\n    return {\"response\": my_app(inputs[\"question\"])}'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='Great! Now we’re ready to run an evaluation. Let’s do it!\\nCopyexperiment_results = client.evaluate(\\n    ls_target, # Your AI system\\n    data=dataset_name, # The data to predict and grade over\\n    evaluators=[concision, correctness], # The evaluators to score the results\\n    experiment_prefix=\"openai-4o-mini\", # A prefix for your experiment names to easily identify them\\n)\\n\\nThis will output a URL. If we click on it, we should see results of our evaluation!\\n\\nIf we go back to the dataset page and select the Experiments tab, we can now see a summary of our one run!\\n\\nLet’s now try it out with a different model! Let’s try gpt-4-turbo\\nCopydef ls_target_v2(inputs: str) -> dict:\\n    return {\"response\": my_app(inputs[\"question\"], model=\"gpt-4-turbo\")}\\n\\nexperiment_results = client.evaluate(\\n    ls_target_v2,\\n    data=dataset_name,\\n    evaluators=[concision, correctness],\\n    experiment_prefix=\"openai-4-turbo\",\\n)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='experiment_results = client.evaluate(\\n    ls_target_v2,\\n    data=dataset_name,\\n    evaluators=[concision, correctness],\\n    experiment_prefix=\"openai-4-turbo\",\\n)\\n\\nAnd now let’s use GPT-4 but also update the prompt to be a bit more strict in requiring the answer to be short.\\nCopyinstructions_v3 = \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"\\n\\ndef ls_target_v3(inputs: str) -> dict:\\n    response = my_app(\\n        inputs[\"question\"],\\n        model=\"gpt-4-turbo\",\\n        instructions=instructions_v3\\n    )\\n    return {\"response\": response}\\n\\nexperiment_results = client.evaluate(\\n    ls_target_v3,\\n    data=dataset_name,\\n    evaluators=[concision, correctness],\\n    experiment_prefix=\"strict-openai-4-turbo\",\\n)\\n\\nIf we go back to the Experiments tab on the datasets page, we should see that all three runs now show up!'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='If we go back to the Experiments tab on the datasets page, we should see that all three runs now show up!\\n\\n\\u200bComparing results\\nAwesome, we’ve evaluated three different runs. But how can we compare results? The first way we can do this is just by looking at the runs in the Experiments tab. If we do that, we can see a high level view of the metrics for each run:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='Great! So we can tell that GPT-4 is better than GPT-3.5 at knowing who companies are, and we can see that the strict prompt helped a lot with the length. But what if we want to explore in more detail?\\nIn order to do that, we can select all the runs we want to compare (in this case all three) and open them up in a comparison view. We immediately see all three tests side by side. Some of the cells are color coded - this is showing a regression of a certain metric compared to a certain baseline. We automatically choose defaults for the baseline and metric, but you can change those yourself. You can also choose which columns and which metrics you see by using the Display control. You can also automatically filter to only see the runs that have improvements/regressions by clicking on the icons at the top.\\n\\nIf we want to see more information, we can also select the Expand button that appears when hovering over a row to open up a side panel with more detailed information:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='\\u200bSet up automated testing to run in CI/CD\\nNow that we’ve run this in a one-off manner, we can set it to run in an automated fashion. We can do this pretty easily by just including it as a pytest file that we run in CI/CD. As part of this, we can either just log the results OR set up some criteria to determine if it passes or not. For example, if I wanted to ensure that we always got at least 80% of generated responses passing the length check, we could set that up with a test like:\\nCopydef test_length_score() -> None:\\n    \"\"\"Test that the length score is at least 80%.\"\"\"\\n    experiment_results = evaluate(\\n        ls_target, # Your AI system\\n        data=dataset_name, # The data to predict and grade over\\n        evaluators=[concision, correctness], # The evaluators to score the results\\n    )\\n    # This will be cleaned up in the next release:\\n    feedback = client.list_feedback(\\n        run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content=')\\n    # This will be cleaned up in the next release:\\n    feedback = client.list_feedback(\\n        run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],\\n        feedback_key=\"concision\"\\n    )\\n    scores = [f.score for f in feedback]\\n    assert sum(scores) / len(scores) >= 0.8, \"Aggregate score should be at least .8\"'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='\\u200bTrack results over time\\nNow that we’ve got these experiments running in an automated fashion, we want to track these results over time. We can do this from the overall Experiments tab in the datasets page. By default, we show evaluation metrics over time (highlighted in red). We also automatically track git metrics, to easily associate it with the branch of your code (highlighted in yellow).'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='\\u200bConclusion\\nThat’s it for this tutorial!\\nWe’ve gone over how to create an initial test set, define some evaluation metrics, run experiments, compare them manually, set up CI/CD, and track results over time. Hopefully this can help you iterate with confidence.\\nThis is just the start. As mentioned earlier, evaluation is an ongoing process. For example - the datapoints you will want to evaluate on will likely continue to change over time. There are many types of evaluators you may wish to explore. For information on this, check out the how-to guides.\\nAdditionally, there are other ways to evaluate data besides in this “offline” manner (e.g. you can evaluate production data). For more information on online evaluation, check out this guide.\\n\\u200bReference code\\nClick to see a consolidated code snippetCopyimport openai\\nfrom langsmith import Client, wrappers\\n\\n# Application code\\nopenai_client = wrappers.wrap_openai(openai.OpenAI())'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='# Application code\\nopenai_client = wrappers.wrap_openai(openai.OpenAI())\\n\\ndefault_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"\\n\\ndef my_app(question: str, model: str = \"gpt-4o-mini\", instructions: str = default_instructions) -> str:\\n    return openai_client.chat.completions.create(\\n        model=model,\\n        temperature=0,\\n        messages=[\\n            {\"role\": \"system\", \"content\": instructions},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    ).choices[0].message.content\\n\\nclient = Client()\\n\\n# Define dataset: these are your test cases\\ndataset_name = \"QA Example Dataset\"\\ndataset = client.create_dataset(dataset_name)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='client.create_examples(\\n    dataset_id=dataset.id,\\n    examples=[\\n        {\\n            \"inputs\": {\"question\": \"What is LangChain?\"},\\n            \"outputs\": {\"answer\": \"A framework for building LLM applications\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is LangSmith?\"},\\n            \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is OpenAI?\"},\\n            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is Google?\"},\\n            \"outputs\": {\"answer\": \"A technology company known for search\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is Mistral?\"},\\n            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\\n        }\\n    ]\\n)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='# Define evaluators\\neval_instructions = \"You are an expert professor specialized in grading students\\' answers to questions.\"\\n\\ndef correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\\n    user_content = f\"\"\"You are grading the following question:\\n{inputs[\\'question\\']}\\nHere is the real answer:\\n{reference_outputs[\\'answer\\']}\\nYou are grading the following predicted answer:\\n{outputs[\\'response\\']}\\nRespond with CORRECT or INCORRECT:\\nGrade:\"\"\"\\n    response = openai_client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\"role\": \"system\", \"content\": eval_instructions},\\n            {\"role\": \"user\", \"content\": user_content},\\n        ],\\n    ).choices[0].message.content\\n    return response == \"CORRECT\"\\n\\ndef concision(outputs: dict, reference_outputs: dict) -> bool:\\n    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='def concision(outputs: dict, reference_outputs: dict) -> bool:\\n    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))\\n\\n# Run evaluations\\ndef ls_target(inputs: str) -> dict:\\n    return {\"response\": my_app(inputs[\"question\"])}\\n\\nexperiment_results_v1 = client.evaluate(\\n    ls_target, # Your AI system\\n    data=dataset_name, # The data to predict and grade over\\n    evaluators=[concision, correctness], # The evaluators to score the results\\n    experiment_prefix=\"openai-4o-mini\", # A prefix for your experiment names to easily identify them\\n)\\n\\ndef ls_target_v2(inputs: str) -> dict:\\n    return {\"response\": my_app(inputs[\"question\"], model=\"gpt-4-turbo\")}\\n\\nexperiment_results_v2 = client.evaluate(\\n    ls_target_v2,\\n    data=dataset_name,\\n    evaluators=[concision, correctness],\\n    experiment_prefix=\"openai-4-turbo\",\\n)\\n\\ninstructions_v3 = \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='instructions_v3 = \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"\\n\\ndef ls_target_v3(inputs: str) -> dict:\\n    response = my_app(\\n        inputs[\"question\"],\\n        model=\"gpt-4-turbo\",\\n        instructions=instructions_v3\\n    )\\n    return {\"response\": response}\\n\\nexperiment_results_v3 = client.evaluate(\\n    ls_target_v3,\\n    data=dataset_name,\\n    evaluators=[concision, correctness],\\n    experiment_prefix=\"strict-openai-4-turbo\",\\n)\\nWas this page helpful?YesNoDynamic few shot example selectionEvaluate a RAG applicationAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Load Data --> Docs --> DDivide our text into chunks --> Embeddings --> Vector DB\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb61b3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v:\\Genai_course\\Langchain\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa4c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstoredb = FAISS.from_documents(documents=documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30a3e528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x2006f496600>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e8e7dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Schema: Each datapoint should consist of, at the very least, the inputs to the application. If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output. Often times you cannot define the perfect output - that’s okay! Evaluation is an iterative process. Sometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent. LangSmith datasets are very flexible and allow you to define arbitrary schemas.\\nHow many: There’s no hard and fast rule for how many you should gather. The main thing is to make sure you have proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value! Don’t worry about getting a large number to start - you can (and should) always add over time!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query from vector sb\n",
    "query = \" Each datapoint should consist of, at the very least, the inputs to the application.\"\n",
    "result = vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac514918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31c9eca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based on the provided context: \\n<context>{context}</context>\\n'), additional_kwargs={})])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000002006F497FB0>, default_metadata=(), model_kwargs={})\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval Chain, Document Chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based on the provided context: \n",
    "<context>{context}</context>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0568f382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, each datapoint should consist of, at the very least, the inputs to the application. It is also very helpful, if possible, to define the expected outputs.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\": \"Each datapoint should consist of, at the very least, the inputs to the application.\",\n",
    "    \"context\":[Document(page_content=\"Each datapoint should consist of, at the very least, the inputs to the application.If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output.\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112bc3e1",
   "metadata": {},
   "source": [
    "However, we want the documents to first come from the retriever we just set up. That wat, we can use the \n",
    "retriever to dynamically select the most relevant documents and pass those in for a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ddb164d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002006F496600>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based on the provided context: \\n<context>{context}</context>\\n'), additional_kwargs={})])\n",
       "            | ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000002006F497FB0>, default_metadata=(), model_kwargs={})\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Input --> Retriever --> Vectorstore Db --> Relevant Docs --> output\n",
    "retriever = vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever_chain = create_retrieval_chain(retriever, document_chain)\n",
    "retriever_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bda3a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is no hard and fast rule for how many datapoints you should gather. The main thing is to ensure proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value, and you can always add more over time.\\n\\nFor the specific tutorial mentioned in the context, they will create 5 datapoints to evaluate.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the response from the LLM\n",
    "response = retriever_chain.invoke({\n",
    "    \"input\": \"Each datapoint should consist of, at the very least, the inputs to the application.\"\n",
    "})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44f4011e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Each datapoint should consist of, at the very least, the inputs to the application.',\n",
       " 'context': [Document(id='b2c4fbda-846c-4110-8662-18cba36fead4', metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='Schema: Each datapoint should consist of, at the very least, the inputs to the application. If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output. Often times you cannot define the perfect output - that’s okay! Evaluation is an iterative process. Sometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent. LangSmith datasets are very flexible and allow you to define arbitrary schemas.\\nHow many: There’s no hard and fast rule for how many you should gather. The main thing is to make sure you have proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value! Don’t worry about getting a large number to start - you can (and should) always add over time!'),\n",
       "  Document(id='3b2a9465-ced1-40d0-8a8a-64c1fffac9b6', metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='And set environment variables to enable LangSmith tracing:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"<Your LangSmith API key>\"\\nexport OPENAI_API_KEY=\"<Your OpenAI API key>\"\\n\\n\\u200bCreate a dataset\\nThe first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:\\n\\nWhat should the schema of each datapoint be?\\nHow many datapoints should I gather?\\nHow should I gather those datapoints?'),\n",
       "  Document(id='4a175a0c-88e7-4dc0-a8a1-08886a28f6c1', metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='# Application code\\nopenai_client = wrappers.wrap_openai(openai.OpenAI())\\n\\ndefault_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"\\n\\ndef my_app(question: str, model: str = \"gpt-4o-mini\", instructions: str = default_instructions) -> str:\\n    return openai_client.chat.completions.create(\\n        model=model,\\n        temperature=0,\\n        messages=[\\n            {\"role\": \"system\", \"content\": instructions},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    ).choices[0].message.content\\n\\nclient = Client()\\n\\n# Define dataset: these are your test cases\\ndataset_name = \"QA Example Dataset\"\\ndataset = client.create_dataset(dataset_name)'),\n",
       "  Document(id='a137e4ec-68e5-4020-a9d6-2e05f91f0d21', metadata={'source': 'https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial', 'title': 'Evaluate a chatbot - Docs by LangChain', 'language': 'en'}, page_content='For this tutorial, we will create 5 datapoints to evaluate on. We will be evaluating a question-answering application. The input will be a question, and the output will be an answer. Since this is a question-answering application, we can define the expected answer. Let’s show how to create and upload this dataset to LangSmith!\\nCopyfrom langsmith import Client')],\n",
       " 'answer': 'There is no hard and fast rule for how many datapoints you should gather. The main thing is to ensure proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value, and you can always add more over time.\\n\\nFor the specific tutorial mentioned in the context, they will create 5 datapoints to evaluate.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda28b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
