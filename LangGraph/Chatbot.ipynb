{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d92b657",
   "metadata": {},
   "source": [
    "#### Chatbots with Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4623ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langgraph langsmith langchain langchain_community langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f419885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "groq_api_key = userdata.get('GROQ_API_KEY')\n",
    "langsmith = userdata.get('LANSMITH_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb66c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d44f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'LanggraphDemo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca5779fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ad0a5",
   "metadata": {},
   "source": [
    "#### Start Building Chatbot using Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee84a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "063281d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    # Messages have the type 'list'. The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in the case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list,add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f67b982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state:State):\n",
    "    return {\"messages\":llm.invoke(state[\"messages\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe8e3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2074462bce0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.add_node(\"chatbot\",chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "529130be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2074462bce0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.add_edge(START,\"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\",END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf7a64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1765905a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGoAAADqCAIAAADF80cYAAAQAElEQVR4nOydCVwTR9/HZzcHhAQIcsglAqIC6gMqKj4qtOLVPvp4lD61Hm+r7Vvl8W7to1bbp2it7WNtfV5ra21rtVa01bZC1SpWqxaxXuAB3lwighGQHCSQZHff2SSEqEl2wya6kv36+cTszOxs9scc/52ZnT+fIAjA0Vb4gIMBnHyM4ORjBCcfIzj5GMHJxwim8pUVa24UKJVynVqhx/QAWFhBCAqPCIAj5BfcEMIDBAYASgYaEwCEDDEcoy2nAQQxpEfg2Yg5kMwZJiEsAg0ZwnAMJ1BgEYhCYwwxpG+9kPEHGOF5IEIhIpEKOsdJegyUAAYgbbP7Cg7LL+Q1qJUYgeMCAeophr/fIBNGWNwGQt4pRpBfcDIc4SGE5SFK3hv8DpMh9+luSADFMt9zi3zk/xZCwAyhoASO3Pd3Qw1HBLC8kPGLEZ4AxfSEvhnX6ghcj4skgsh48dP/CACO47B8hYfkZw7VYTgIDPPoPyygU5wHeJJR1RN/5Nytut6o1+ORPSSj/qejQ6c7Jt+3K8rVKjw+WZoyvgNoX1w+1Zi/R4ZjxP++Gw0EdM9yQL7PFt4IihClzw0D7ZcjO2svnZT/dUxAYqovnfR05Vv/xo2n0oMZNrRPCp8tLJmyJNLHn0eZkpZ8MLvXVnbhP9mtnGN8sbg0Kc2/73CKMogCKjYsKh36j2C30g4y44Pok7m1ChluPxmFfFuWVwSFe8T2FwP3Y8BI/20fldlPY0++MwcbGlX6CXPac19hh77DpF7e/B/XVdlJY1e+Q/W9kqXAjXl+XkRNucZOApvynT+iIPTEkAn+wI3x8kFEEt5Pn9osgDblK/zjXlCEJ3i0DB8+vKqqytGzSkpKRo8eDVxDQopUdqvJVqxN+eAQQL+RgeARUl1dfe/ePeA4ly5dAi6jb5ofrgc3r6itxlofcSkpbIRP8RHdhcAFQEtz+/bte/bsqaioiIqKSk5OzsjIKCwsnDlzJowdO3ZsamrqmjVrYJnatWvX6dOnb9++HR0dPW7cuPT0dGMOaWlpr7766uHDh+FZU6dO3bp1KwxMSkpasGDB5MmTgbPxFPOKjisjYr0ejrIhX3Gj6wy9HTt2bNq0af78+YMGDTpy5Mj69evFYvG0adPWrl0LA7Ozs8PCyL4eKgiFW7p0KRzKKS8v//DDD0NCQuApMEogEPz888/9+/eHIvbt2xcmyM3NhX8P4Bokvvx6WbPVKOvyKep1nl7Ujyxto6CgID4+3thajR8/vl+/fmq1laqxatWqxsbG0NBQYChZOTk5+fn5RvmgXr6+vgsXLgSPBN8AYVWpI5VX24QJhK6SLyEhYd26dcuXL+/du3dKSkp4eLjVZLCOw3J6/PhxWMeNIcZSaQT+AcCjwlOCaJsxq1HW5cMxHKV+nGsjkyZNgrX16NGjmZmZfD4f9rZz584NDLyvm8JxfN68eVqtdvbs2bDoeXt7v/LKK5YJhEKXtMtWIQeCEcRqlHX5PL2EzWrrejMHRdHxBkpLS0+dOrVx40aVSvXJJ59Yprly5UpxcfFnn30GGzhjiFKpDAoKAo+DJhWB8hyRT+zLk9+13lgyB7bxcXFxXbp0iTYAdYH9wANpGhoa4KdZr1ID8BTwOJDXaQUe1psy61U0Mk6ibaYYbGgz+/fvf/PNN48dOyaXy/Py8qD9AVtD8qKRkfDz4MGDRUVFUFZYr6FFolAoYLe7evVqaN9Aw9BqhhEREbW1tbATN7eSzkV5Tyf1tz4AbV2+uAFiOGxdV60FLmDZsmVQnddffx2abytWrIBWHrROYDjsQ8aMGbNhwwbYsQQHB7/33nsXL14cOnQotOZmzZoFjT4oq9n0s2Tw4MGJiYmwIz5w4ABwAWqlvnsf6+PENodLv1xaGhTuOTYjFLg3V06rftteM/vjGKuxNvvXbn18bBk7bsXJ/XV+QTZ7eZvT5KnPBRTlN5w7Krc1aVJTUzNx4kSrURKJBHamVqNgtYWPHMA1bDZgNQoaH7bqGbSNrLYJRhT12hnvx9iKtTfX8VuW7Po5ZcZ/rPd3er1eJpNZjWpqavL0tD5aAzsE19kfSgNWo2AX5OPjYzUKhsO/t9Wo71ZWwFn5qW93BjagmCr6Yklp51ivUS8FA/fj5tWmXzbemrUmxk4aimeLGauiSy6qmpTuuIB336bbg8dTVBTqR7PhLwZvfq8MuBnfvFsR3lWcMNjHfjJa87z3anRZq2/OWvN4jP5Hz4ZFZSkTAuMHUK8JoLvKoPySes/XtxMG+w0Z355nP25e1uzbfLtzrOSZabTWCjmyRAgDXywr5QuQZ14KCe3yqKdBHgFZ/6mU39UOHB2UmOpN8xSHF6jt/aq64qraQ4R2TfROmdCWNXFs49wxxcW8BkWdNiDU84U3wh06t43LI/duqqm6rtbpCD4fiLz53lKB0JNcDHn/8khD/sYFiuQyRjhUBQfyyO/QiMVxwnSIAsNiUAIgADWEA3JQi8wBN4xaoGhrIG5YfQrzRHkoHJQkA3lwdJL8JNeo4mQawriE0pAbIIPJRZrkhYxrW8lTeDot1qTEG5W6Zg3O4yEdQoTPZ4QDx4cQ2yifkcZ64s/c2ruVTZpGTK+DQ5wIbikf+eNxgkAtQ8jFtYBAyWcA0yGZjDCI15LAdK5hiS7MlMdDWwPN0qBmcU1/EgBacgP3ZWJcd0pGtSzRRRHA90BFEr5fIL/XX/3Cu7d9WoeRfI+AkSNHZmVl+fuztL9i+8p6+GgIn/MAW+HkYwQnHyPYLp9Op4OT4oCtsFo+3NC5oq6bM2UMq+Vjec0FnHwMYfWPY3nDB7jSxxBOPkZw8jGCk48RbJeP6zraDlf6GMHJxwhOPkZAs5mTr+1wpY8RnHyM4ORjBCcfI7gRF0ZwpY8RPB7P25vucpPHAtuniuRyOWAx7K4afD6sv4DFcPIxgpOPEZx8jODkYwTbDRdOvrbDlT5GcPIxgpOPEZx8jODkYwQnHyM4+RjByccITj5GsF8+Nr5VlJmZmZOTY/xhpPMDAyiKnj59GrAMNi5az8jIiIyMRA3Ax174CeWztdHa44WN8gUFBQ0bNswyBMo3duxYwD5Y+srElClTOndu3f4jLCxs3LhxgH2wVD44wTZmzBjzCzEjRoyQStm4gzR7X9iZNGmSsb0LDQ2dMGECYCWO9bw3zmnKilVNap2tBHwhotfazJDHRzC99ViEhxLYg1veVVVV3Si5ERIaEhcbi+mtb4jH5yN6a3ma31l/OEogQHQ6K+EenvyOEZ4JqRSbj9x3FZryaTQg6/1ynRYTePC0Gptb+/GEBKZFbMYKAGZL+Ra3TA+AEziCQqsFIWxsZonyAW7VNERNr+0D2j9S6IViWvKEIeOD4/t7ARrQMpu1GrD532Xx/Xz7jGhvPnYepuxi47GfagT84K59qBWkVfq+WFSa8lx4ePdHt9vqY2fbyrLnMiIDoxD7yai7jtwtMqEn3620gwSEeR7YXkmZjFq+O7eafANZvUrMFXSOFzcqqfcOppYPdhQ4QICbweOjmI5681vqrgPDCJzdwx6uAPb4GEbdK3AuPhnByWcTOg0WDflQ92v5jNrRuG0a8uHADbfeJFo2wrIPV3kZwclnHdJfMOKMnhdBAeJ+jR+55x9BfdvU8hE4YPceda7COT2ve5Y+AGh1mFzpswniHMPFLSHMH3ahHjJAnGc2P//CM199vR4wYOz4tG+3fgVcj2FDVer7ppaPeNxmc+byxft+zQYM+Hn3D6s+/DdwAeydaTNz9SpTF5RtyAFx2jOv42AYtnPXti3fboTf4+N6vfzSjF69Ek3X4wt++vn7DV+sFQqFPXsmLlm83NeHdKdy4sQfh38/cOFioUIhj4vtOXXqq70Tk2D402nk5+qPVny+4ZNfso8YM4Glaf/+nKrblX169399wVtSqZ8xHNbrA7l7amtlQUHBiQl9F8xfAmeK57/+2vnzBTD24oXCrG05gC6GLZKpcEnp2/jluuzsncszP1r21srAwI6Llsy5ebPcGHX02G+NjaoPP1j35sJ3iorOffPN58DgHmXlqmXNzc2LF2W+v3JtRETk0mUL6uvrYNT+fcfh55sL3zZr9+uv2ffu1c2cOX/pkvfOnTvz6fqPjOHfbN6wO/uHjBnzd+088Mr0fx45ehD+CWH42o83xsX1HDHib45oBwh65c/5Iy5yhfyHnd/Nn7e4X1IyPBwwYJBa3VhXXwtFgYdeXuKpU0z+/o7nH4XFDX7x9PT8auMOkUjk60suJYClLztn18Wic6kpaQ/nL/LymvbyTMRgVowePWHXj1larbZZ27x9x5aMmQsGD34Khj+VOqy09Pp3276eMH6iS99Hd/6IS3lZCfyMje1hugCfvzxztTm2V89E83dfH6m22eRMD0r81defnjt/tq6u1hjS0GDdV29S32SkxSSLj++l26GrrbsLE+t0OljKzMm6dYtTqVRVVZWRkdGgTdCx++gZLo4UP5WK9Bbk6WHTV1Frzi353rlTM2/Bq/D+3176fu7+EwcP/Gk7e7L8mr+LRORUrFzeUF9f+8BFjVEaDQNnX04ZsHL0qUMsJr2EwNJE/xTYTsEKCBs+WH+B7XJnpKmp1dc6bEbhJ6zyxkCNRZTxB3To0EafDiZvAVQ4v+uIiekOi9j5CwXGQzgNv/iteQcO2PM9DHtbb28fo3aA7F4O2Ul848ZV83dokcAePDAgqEuXbjwer7j4vDnq8uUib4l3YGAbvXIZ+l1nmM2OPnVIJJLhw56FPe+v+3MKz51Z9+nqs2dPWrZKDxMd3RU2eTm//KjX60+eyi8oOAULlExWA6M8PDygBGfO/AmzMq5zLisvgV0TtI2uXb8CzZSUIUNh5+Dj7QMv+t22Tfn5xxRKRW7u3p93f5+ePtm4xC0srBNUs7j4AnA29CovcIx5cxet/e8Haz5eCW8ypku35e+uNna7tkgbOrKiovTbrV9+snYV7K8X/evdHd9/m7V9s1KpgGbd5EnToVFy6nT+9qw9er3uxYkvQSE+37BWLBb3Sxo4e5bJR/Ssf74BxVqx8i2ocmho+KQXp8GUxqgxf5tw7drl9z94Z9vW3cCpUK9x2bikTNpR8Mw0Ni4tdh1XzyhO7JHN+STGfjJuxMUG6ON7aGsPEE4aLkXccp6XJjS6Dpa743EZTqq8hJsWPudUXg47cPLZBEWcMmDlrh0HTjhjlYE7LhCiDVd5GcHJxwhOPkZw8jGCWj6BCBEKn4DpYOeCoKiAxl1TyyeW8NUqt+t966ub6chHnSJhiL+yvgm4GbeuKUMiRZTJqOXr3k/kE+Cxaw31C17thoNb72A64tlXqL27032f97ftd8svNQZ3FoXGSHD8wZe9EOOKpIdztzS6EdPcvSmEMD3PIPcb5qaMEFO8lVhgF5Ob6Qeva8yNQFoXLCOtP8GUmMcj6m5jldeUAiFvyhJao+sOvE1+PKf+WoFC24xrmx582QtBDK7+mQAACCBJREFUHhxfNP04pFVUsyttk+dr0s84SlgoYj6l9W7v/wSmu73PE7flhR5QxFqUacW35Q82Z84Xwk6SHxLl+ex06nLnsHyPhVGjRm3bto1zrt1GOPfGjODkYwTLvT1xpY8RrJYPdms4jvN4PMBWOG8xjODkYwTn6okRXOljBCcfIzj5GMG1fYzgSh8jOPkYwcnHCE4+RnDyMYKTjxGcfIzg5GMEZzYzgit9jODkYwTbvcUEBgYCFsNq+TAMk8lkgMVwvooYwcnHCE4+RnDyMYKTjxGcfIxgu3zQdgEshit9jODkYwTb5YODLoDFcKWPEZx8jODkYwQnHyM4+RjByccINr5VNGfOnLy8PPPWnCiK4jgOD8+ePQtYBhvfc543b154eDjaAjAoGBERAdgHG+WLiYkZPHiwZbWARS81NRWwD/Y61+7UqZP5EH5PT08H7IOl8oWFhaWlmfa8hg1fUlKS0VM022DvHg8TJ040eneHny+88AJgJc40XJQy4k61WqvBLF0ym14KN78XbXynvOWQQMh/wPyed8ury4aUHiMGvvp705GeXeM1ssAimcIiu5asW7ME1g7uex+bjwK+kCftKAgIdZqzXKaGy/XCxrMH6xvqtDotYXQHTr4njpHbHiMtrvYIw3WM+wAi5AURC5VwUw1oSdD6y1pfAgfmfcgsjlt1tjz3vnASwnIPM/NL5HwB4u0n6N7Hu99IP8CAtsv3+w+1V88o9BghFPEkHbz8Qr1Fvk+GC2R9M1Z/S6mqUzc36uDth0WLxmaEgjbRFvnu3dLt/LQSCucX5hvSndFf77HTUKW+U1KH6bE+QzskP+PwvTgsX+5W2bVChX+YNCT+yRbOkoZqze3Ld3w7CCYvccw4d0y+Q9/XXi9Uxqay8QGAOdfzq3goPj0zkv4pDsi3+7Pq2+VN8U+3T+2MXD9eJeARL2d2ppmert23b1NNTWU71w7SdVAYQHmbl1fQTE9LvrIiTVlxY2xKO9fOSGT/kGYN8euWO3QS05Iv97vqwCgpcBu6p3QquaCik5Javn2bZAjKC+riRvJBxL6eW1ZQV2Fq+SouKwOj24+NQpOofsGqBr1cRrFEhEK+E3vr4ZOOX5gYsBJV472Fbw84d/E34AKEXvzcrGr7aSjku1ag9JA8GY9iTscvxKeuWms/DYV8agXWIdQXuCUBUT56PVFfY6/+2huwapDBkUpcGuYFXINCWffLr2vLKy9otU3duyYPS50eFEjaq9V3StZ8OmnujE2Hj20punzU1ycosdfwZ4fPMm4nVHghd/+hLzQaRXzskNRBk4Er4fHQoryGlHSb29/ZK30lF5XAZTvWYxi2YdM/S8oLnhuz+I3ZWRJxh//bOL227haM4vPIF7F2Zq/q/ZeRH/w7b1J65tHj284Xkw1c9Z0bWbveSer97OL5PyYl/i177xrgSlA+Wlttb99We/Ip7+lQnqvkK7t5TlZb/mJ6Zmy3gT7e/mNGzRV7Sf84scOcIKHH0ISeaXy+oEtUH3+/sFtVV2Bg/skfpb7Bw596xcvLJya674CkccClILhaZW+Jl73Kq9XiBO6qWeDyivM8nqBrdJLxEI6zQplKywvNCcJD48zfPT29NU2k78ba+srgjq0+JzuFxQOXAod+9fYKkD35hALUdXPomiYVhumg2WEZKBG3GpgIYqVmqNWKAP/WGTihkHpvYEbgCMpvq3z+oR6Iq+ou8Jb4w5ufPvm+xss4KW4HWGd1utbGqLnZAU+YbYDAcJHY3hux9uTrluh97CdaT85tICykm1arkUo7BnQwzUDW1VdZlj6r+ElDLl35A9oDRqEvXc0DrgTHQWiUvQJu76/tIYaDN2htuQK4gK5d+sV2Hbhz98p7DTWqxobjJ3f9d8PLpwp+sX9WQo9h8Elj9941cJjyRunZ/JO7gCvBMTwxrYOdBBQTlRIpv6FaFRDpA1zA9Ckfnzj903c/LKuovBgY0LlPwqghAynmc7t3HTB65JwTp356851k2AVPfj5z/VczXOTQ5s7VBoGQJ7Jr9VKMNl84qsjbUxs/lO7oa3viWl5lx3Ch/Uk4iqb6L6k+KA/IbjQA90Or0VNOYFKvMujWx/vqWXlQjPXxPtiKv7NquNUovV4LLTvEWucdHBg9+7UvgfP4euvrZTfPW43S6ZoFAo+Hw4UCz3f+tRfYoOTP235B1GMltKaKvlxa5uUnCethvRFVKGqthjdrNR427DIejy8WO3P8tVEtx/TWHw80zY0iD2sDbggCn3asn6LQl56unPVRDKCClnxaNfjy7ZIewyKBe3DpcHnCEL9Bf+9AmZLWXIfQC/R92v/SYbrzT08014/f6hDsQUc7QH+iMnm0tHeqtPhQOWjXXD5y0z+YP/ENumsJHVtlUPC74s89d2P+Gg4HskG748qRmwGhwvR5YfRPcXiNS+Fhef7eu15SUVRSMGgvVF+pr69UdOou/vsMx26qjQvUNmeWqxR6ib8osveTLeLty/XyGiUcxv77a+HBUQ7P6rR9fd/1QvUf2bJGuZ7HRz28+N6BEp+OYk8J2yt1sxrT1DfLZSqNqhnTYkIPJD5ZSrOjeBjGr8XgYN/mmqoSjbYJN2aFwCFa4qHVuXYxL0W1fxL9QDuXgmY8nMEQCNGAMGHyM/7BUR6AAc5/q0ijIicyLC9B3iMKx20fvBCBQqVbk5iAI1FwnMhyUTKw5jnKnGGr86iH/EqhhrW/ZnhAJOIBp3qvYLurJ5bTDu2PRwknHyM4+RjByccITj5GcPIx4v8BAAD//2pcLOUAAAAGSURBVAMAFowuycKA8vEAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6766498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([{'messages': AIMessage(content='Hello! How can I help you today?', additional_kwargs={'reasoning_content': 'We need to respond as ChatGPT. The user just says \"hi\". We greet back.'}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 72, 'total_tokens': 110, 'completion_time': 0.081581188, 'prompt_time': 0.006641137, 'queue_time': 9.349467591, 'total_time': 0.088222325}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_91af62a853', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--9307a133-bc2b-43e1-8ef0-dc27a1947156-0', usage_metadata={'input_tokens': 72, 'output_tokens': 38, 'total_tokens': 110})}])\n",
      "content='Hello! How can I help you today?' additional_kwargs={'reasoning_content': 'We need to respond as ChatGPT. The user just says \"hi\". We greet back.'} response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 72, 'total_tokens': 110, 'completion_time': 0.081581188, 'prompt_time': 0.006641137, 'queue_time': 9.349467591, 'total_time': 0.088222325}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_91af62a853', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--9307a133-bc2b-43e1-8ef0-dc27a1947156-0' usage_metadata={'input_tokens': 72, 'output_tokens': 38, 'total_tokens': 110}\n",
      "Assistant Hello! How can I help you today?\n",
      "dict_values([{'messages': AIMessage(content='## What Is Machine Learning?\\n\\nMachine Learning (ML) is a branch of artificial intelligence that gives computers the ability to **learn from data** and improve their performance on a task without being explicitly programmed for every possible scenario. Instead of writing step‑by‑step rules, we feed the system examples, let it discover patterns, and then use those patterns to make predictions or decisions on new, unseen data.\\n\\n---\\n\\n## Core Concepts\\n\\n| Concept | What It Means | Example |\\n|---------|----------------|---------|\\n| **Data** | The raw information (numbers, text, images, etc.) that the model learns from. | A table of house prices, a collection of photos, a log of user clicks. |\\n| **Features** | Individual measurable properties extracted from the data. | Square footage, number of bedrooms, pixel intensities. |\\n| **Label / Target** | The outcome we want the model to predict (only in supervised learning). | Sale price, “cat vs. dog”, next‑day stock return. |\\n| **Model** | A mathematical function (or a network of functions) that maps features → predictions. | Linear regression line, decision tree, neural network. |\\n| **Training** | The process of adjusting the model’s parameters so it fits the training data. | Gradient descent updating weights in a neural net. |\\n| **Loss / Cost Function** | A metric that quantifies how far the model’s predictions are from the true labels. | Mean Squared Error, Cross‑Entropy. |\\n| **Optimization** | An algorithm (e.g., gradient descent) that minimizes the loss by tweaking parameters. |\\n| **Evaluation** | Measuring model performance on data it hasn’t seen during training (validation / test sets). | Accuracy, F1‑score, ROC‑AUC, RMSE. |\\n| **Generalization** | The ability of a model to perform well on new, unseen data. |\\n| **Overfitting / Underfitting** | Overfitting: model memorizes training data, performs poorly on new data. <br>Underfitting: model is too simple to capture underlying patterns. |\\n| **Regularization** | Techniques (L1/L2 penalties, dropout, early stopping) that discourage overfitting. |\\n\\n---\\n\\n## Main Types of Machine Learning\\n\\n| Type | How It Works | Typical Tasks |\\n|------|--------------|---------------|\\n| **Supervised Learning** | Learns a mapping from **features → labels** using a labeled dataset. | Classification (spam vs. not spam), Regression (predict house price). |\\n| **Unsupervised Learning** | Finds structure in **unlabeled** data—no explicit target. | Clustering (customer segmentation), Dimensionality reduction (PCA, t‑SNE). |\\n| **Semi‑Supervised Learning** | Uses a small amount of labeled data plus a large amount of unlabeled data. | Improving image classifiers when labeling is expensive. |\\n| **Reinforcement Learning (RL)** | An **agent** interacts with an environment, receives rewards, and learns a policy to maximize cumulative reward. | Game playing (AlphaGo), robotics, recommendation systems. |\\n| **Self‑Supervised Learning** | Creates its own supervision signal from raw data (e.g., predicting masked words in a sentence). | Large‑scale language models (GPT, BERT), vision transformers. |\\n\\n---\\n\\n## Common Algorithms (A Quick Tour)\\n\\n| Category | Algorithm | When It’s Useful |\\n|----------|-----------|------------------|\\n| **Linear Models** | Linear Regression, Logistic Regression | Interpretable baselines; small‑to‑medium data. |\\n| **Tree‑Based** | Decision Trees, Random Forests, Gradient Boosting (XGBoost, LightGBM, CatBoost) | Tabular data, handling mixed feature types, strong performance out‑of‑the‑box. |\\n| **Support Vector Machines (SVM)** | Linear / kernel SVM | High‑dimensional spaces, limited data, clear margin. |\\n| **Nearest Neighbors** | k‑Nearest Neighbors (k‑NN) | Simple, non‑parametric; works well for low‑dimensional data. |\\n| **Neural Networks** | Feed‑forward (MLP), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Transformers | Image, speech, text, sequential data; large datasets & GPUs. |\\n| **Clustering** | k‑Means, DBSCAN, Hierarchical Clustering | Discover groups in unlabeled data. |\\n| **Dimensionality Reduction** | Principal Component Analysis (PCA), t‑SNE, UMAP | Visualisation, noise reduction, speed‑up. |\\n| **Reinforcement Learning** | Q‑Learning, Deep Q‑Network (DQN), Proximal Policy Optimization (PPO) | Game AI, robotics, control problems. |\\n\\n---\\n\\n## Typical Machine‑Learning Workflow\\n\\n1. **Define the Problem** – classification, regression, clustering, etc.\\n2. **Collect & Explore Data** – gather raw data, visualize, compute statistics.\\n3. **Preprocess** – clean missing values, encode categorical variables, scale/normalize, augment (for images/audio).\\n4. **Feature Engineering** – create informative features, dimensionality reduction, feature selection.\\n5. **Split Data** – training set (≈70‑80\\u202f%), validation set (≈10‑15\\u202f%), test set (≈10‑15\\u202f%).\\n6. **Choose a Model** – start with simple baseline, then try more complex algorithms.\\n7. **Train** – feed the training data, tune hyper‑parameters (learning rate, depth, regularization strength, etc.).\\n8. **Validate** – monitor performance on validation set; use techniques like cross‑validation.\\n9. **Test** – final unbiased evaluation on the hold‑out test set.\\n10. **Deploy** – serve the model via API, embed in an app, or schedule batch predictions.\\n11. **Monitor & Maintain** – track drift, retrain with new data, update as needed.\\n\\n---\\n\\n## Practical Tips for Success\\n\\n| Tip | Why It Helps |\\n|-----|--------------|\\n| **Start Simple** | Baseline models (logistic regression, random forest) give a performance reference and are easy to interpret. |\\n| **Feature Importance** | Understanding which features drive predictions aids debugging and stakeholder trust. |\\n| **Cross‑Validation** | Reduces variance in performance estimates, especially when data is limited. |\\n| **Hyper‑Parameter Search** | Grid search, random search, or Bayesian optimization can uncover better configurations. |\\n| **Regularization & Early Stopping** | Prevents overfitting, especially for high‑capacity models (deep nets). |\\n| **Data Quality > Model Complexity** | Clean, representative data often yields bigger gains than a fancier algorithm. |\\n| **Explainability** | Tools like SHAP, LIME, or feature importance plots help explain decisions to non‑technical audiences. |\\n| **Ethics & Bias Checks** | Inspect training data and model outputs for unfair bias; consider fairness metrics. |\\n| **Scalability** | For large datasets, use distributed training frameworks (Spark MLlib, TensorFlow Distributed, PyTorch DDP). |\\n| **Version Control** | Keep track of data, code, and model versions (e.g., DVC, MLflow). |\\n\\n---\\n\\n## Real‑World Applications\\n\\n| Domain | Example Use Cases |\\n|--------|-------------------|\\n| **Healthcare** | Disease diagnosis from medical images, drug discovery, patient risk scoring. |\\n| **Finance** | Credit scoring, fraud detection, algorithmic trading, portfolio optimization. |\\n| **Retail & E‑Commerce** | Product recommendation, demand forecasting, dynamic pricing. |\\n| **Transportation** | Route optimization, autonomous driving perception, traffic prediction. |\\n| **Natural Language Processing** | Sentiment analysis, chatbots, machine translation, summarization. |\\n| **Computer Vision** | Face recognition, object detection, video analytics, quality inspection. |\\n| **Manufacturing** | Predictive maintenance, defect detection, process optimization. |\\n| **Entertainment** | Content recommendation, music generation, video game AI. |\\n\\n---\\n\\n## Current Trends & Future Directions\\n\\n| Trend | What It Means |\\n|-------|----------------|\\n| **Foundation Models** | Huge, pre‑trained models (e.g., GPT‑4, CLIP, DALL·E) that can be fine‑tuned for many downstream tasks. |\\n| **Self‑Supervised Learning** | Learning useful representations without labels, reducing reliance on expensive annotation. |\\n| **Edge & TinyML** | Deploying ML models on low‑power devices (IoT, smartphones) with quantization and model compression. |\\n| **Explainable AI (XAI)** | Growing emphasis on transparent, interpretable models for regulated industries. |\\n| **Responsible AI** | Systematic frameworks for fairness, privacy (differential privacy), and robustness against adversarial attacks. |\\n| **AutoML** | Automated pipeline building (feature engineering, model selection, hyper‑parameter tuning) to democratize ML. |\\n| **Hybrid AI** | Combining symbolic reasoning with neural networks for better reasoning and data efficiency. |\\n\\n---\\n\\n## Quick Glossary\\n\\n- **Epoch** – One full pass through the training dataset.\\n- **Batch / Mini‑batch** – Subset of training samples used to compute a gradient step.\\n- **Learning Rate** – Step size for updating model parameters during optimization.\\n- **Embedding** – Dense vector representation of high‑dimensional categorical or textual data.\\n- **Dropout** – Randomly zeroes out neurons during training to reduce overfitting.\\n- **Transfer Learning** – Re‑using a model trained on one task for another, often with fine‑tuning.\\n- **Ensemble** – Combining multiple models (e.g., bagging, boosting) to improve performance.\\n\\n---\\n\\n### TL;DR\\n\\nMachine learning lets computers learn patterns from data instead of following hand‑written rules. It works by training models—mathematical functions—on labeled (supervised) or unlabeled (unsupervised) data, optimizing a loss function, and evaluating how well the model generalizes to new inputs. The field includes a rich toolbox of algorithms (linear models, trees, neural nets, clustering, RL, etc.), a standard workflow (collect → clean → feature → train → evaluate → deploy), and countless applications across industry and research. Staying mindful of data quality, model interpretability, and ethical considerations is as important as mastering the algorithms themselves.', additional_kwargs={'reasoning_content': 'The user asks \"explain about machine learning?\" Simple request. Provide an overview: definition, types (supervised, unsupervised, reinforcement), key concepts (features, labels, models, training, loss, overfitting, regularization), algorithms (linear regression, decision trees, SVM, neural networks), applications, pipeline, challenges, future. Should be concise but thorough. No disallowed content. Provide friendly explanation.'}, response_metadata={'token_usage': {'completion_tokens': 2266, 'prompt_tokens': 77, 'total_tokens': 2343, 'completion_time': 4.849267562, 'prompt_time': 0.003161482, 'queue_time': 5.077415806, 'total_time': 4.852429044}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_5c888d599f', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--8cedfe83-a88f-4e7a-a609-8be5e34ba5d4-0', usage_metadata={'input_tokens': 77, 'output_tokens': 2266, 'total_tokens': 2343})}])\n",
      "content='## What Is Machine Learning?\\n\\nMachine Learning (ML) is a branch of artificial intelligence that gives computers the ability to **learn from data** and improve their performance on a task without being explicitly programmed for every possible scenario. Instead of writing step‑by‑step rules, we feed the system examples, let it discover patterns, and then use those patterns to make predictions or decisions on new, unseen data.\\n\\n---\\n\\n## Core Concepts\\n\\n| Concept | What It Means | Example |\\n|---------|----------------|---------|\\n| **Data** | The raw information (numbers, text, images, etc.) that the model learns from. | A table of house prices, a collection of photos, a log of user clicks. |\\n| **Features** | Individual measurable properties extracted from the data. | Square footage, number of bedrooms, pixel intensities. |\\n| **Label / Target** | The outcome we want the model to predict (only in supervised learning). | Sale price, “cat vs. dog”, next‑day stock return. |\\n| **Model** | A mathematical function (or a network of functions) that maps features → predictions. | Linear regression line, decision tree, neural network. |\\n| **Training** | The process of adjusting the model’s parameters so it fits the training data. | Gradient descent updating weights in a neural net. |\\n| **Loss / Cost Function** | A metric that quantifies how far the model’s predictions are from the true labels. | Mean Squared Error, Cross‑Entropy. |\\n| **Optimization** | An algorithm (e.g., gradient descent) that minimizes the loss by tweaking parameters. |\\n| **Evaluation** | Measuring model performance on data it hasn’t seen during training (validation / test sets). | Accuracy, F1‑score, ROC‑AUC, RMSE. |\\n| **Generalization** | The ability of a model to perform well on new, unseen data. |\\n| **Overfitting / Underfitting** | Overfitting: model memorizes training data, performs poorly on new data. <br>Underfitting: model is too simple to capture underlying patterns. |\\n| **Regularization** | Techniques (L1/L2 penalties, dropout, early stopping) that discourage overfitting. |\\n\\n---\\n\\n## Main Types of Machine Learning\\n\\n| Type | How It Works | Typical Tasks |\\n|------|--------------|---------------|\\n| **Supervised Learning** | Learns a mapping from **features → labels** using a labeled dataset. | Classification (spam vs. not spam), Regression (predict house price). |\\n| **Unsupervised Learning** | Finds structure in **unlabeled** data—no explicit target. | Clustering (customer segmentation), Dimensionality reduction (PCA, t‑SNE). |\\n| **Semi‑Supervised Learning** | Uses a small amount of labeled data plus a large amount of unlabeled data. | Improving image classifiers when labeling is expensive. |\\n| **Reinforcement Learning (RL)** | An **agent** interacts with an environment, receives rewards, and learns a policy to maximize cumulative reward. | Game playing (AlphaGo), robotics, recommendation systems. |\\n| **Self‑Supervised Learning** | Creates its own supervision signal from raw data (e.g., predicting masked words in a sentence). | Large‑scale language models (GPT, BERT), vision transformers. |\\n\\n---\\n\\n## Common Algorithms (A Quick Tour)\\n\\n| Category | Algorithm | When It’s Useful |\\n|----------|-----------|------------------|\\n| **Linear Models** | Linear Regression, Logistic Regression | Interpretable baselines; small‑to‑medium data. |\\n| **Tree‑Based** | Decision Trees, Random Forests, Gradient Boosting (XGBoost, LightGBM, CatBoost) | Tabular data, handling mixed feature types, strong performance out‑of‑the‑box. |\\n| **Support Vector Machines (SVM)** | Linear / kernel SVM | High‑dimensional spaces, limited data, clear margin. |\\n| **Nearest Neighbors** | k‑Nearest Neighbors (k‑NN) | Simple, non‑parametric; works well for low‑dimensional data. |\\n| **Neural Networks** | Feed‑forward (MLP), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Transformers | Image, speech, text, sequential data; large datasets & GPUs. |\\n| **Clustering** | k‑Means, DBSCAN, Hierarchical Clustering | Discover groups in unlabeled data. |\\n| **Dimensionality Reduction** | Principal Component Analysis (PCA), t‑SNE, UMAP | Visualisation, noise reduction, speed‑up. |\\n| **Reinforcement Learning** | Q‑Learning, Deep Q‑Network (DQN), Proximal Policy Optimization (PPO) | Game AI, robotics, control problems. |\\n\\n---\\n\\n## Typical Machine‑Learning Workflow\\n\\n1. **Define the Problem** – classification, regression, clustering, etc.\\n2. **Collect & Explore Data** – gather raw data, visualize, compute statistics.\\n3. **Preprocess** – clean missing values, encode categorical variables, scale/normalize, augment (for images/audio).\\n4. **Feature Engineering** – create informative features, dimensionality reduction, feature selection.\\n5. **Split Data** – training set (≈70‑80\\u202f%), validation set (≈10‑15\\u202f%), test set (≈10‑15\\u202f%).\\n6. **Choose a Model** – start with simple baseline, then try more complex algorithms.\\n7. **Train** – feed the training data, tune hyper‑parameters (learning rate, depth, regularization strength, etc.).\\n8. **Validate** – monitor performance on validation set; use techniques like cross‑validation.\\n9. **Test** – final unbiased evaluation on the hold‑out test set.\\n10. **Deploy** – serve the model via API, embed in an app, or schedule batch predictions.\\n11. **Monitor & Maintain** – track drift, retrain with new data, update as needed.\\n\\n---\\n\\n## Practical Tips for Success\\n\\n| Tip | Why It Helps |\\n|-----|--------------|\\n| **Start Simple** | Baseline models (logistic regression, random forest) give a performance reference and are easy to interpret. |\\n| **Feature Importance** | Understanding which features drive predictions aids debugging and stakeholder trust. |\\n| **Cross‑Validation** | Reduces variance in performance estimates, especially when data is limited. |\\n| **Hyper‑Parameter Search** | Grid search, random search, or Bayesian optimization can uncover better configurations. |\\n| **Regularization & Early Stopping** | Prevents overfitting, especially for high‑capacity models (deep nets). |\\n| **Data Quality > Model Complexity** | Clean, representative data often yields bigger gains than a fancier algorithm. |\\n| **Explainability** | Tools like SHAP, LIME, or feature importance plots help explain decisions to non‑technical audiences. |\\n| **Ethics & Bias Checks** | Inspect training data and model outputs for unfair bias; consider fairness metrics. |\\n| **Scalability** | For large datasets, use distributed training frameworks (Spark MLlib, TensorFlow Distributed, PyTorch DDP). |\\n| **Version Control** | Keep track of data, code, and model versions (e.g., DVC, MLflow). |\\n\\n---\\n\\n## Real‑World Applications\\n\\n| Domain | Example Use Cases |\\n|--------|-------------------|\\n| **Healthcare** | Disease diagnosis from medical images, drug discovery, patient risk scoring. |\\n| **Finance** | Credit scoring, fraud detection, algorithmic trading, portfolio optimization. |\\n| **Retail & E‑Commerce** | Product recommendation, demand forecasting, dynamic pricing. |\\n| **Transportation** | Route optimization, autonomous driving perception, traffic prediction. |\\n| **Natural Language Processing** | Sentiment analysis, chatbots, machine translation, summarization. |\\n| **Computer Vision** | Face recognition, object detection, video analytics, quality inspection. |\\n| **Manufacturing** | Predictive maintenance, defect detection, process optimization. |\\n| **Entertainment** | Content recommendation, music generation, video game AI. |\\n\\n---\\n\\n## Current Trends & Future Directions\\n\\n| Trend | What It Means |\\n|-------|----------------|\\n| **Foundation Models** | Huge, pre‑trained models (e.g., GPT‑4, CLIP, DALL·E) that can be fine‑tuned for many downstream tasks. |\\n| **Self‑Supervised Learning** | Learning useful representations without labels, reducing reliance on expensive annotation. |\\n| **Edge & TinyML** | Deploying ML models on low‑power devices (IoT, smartphones) with quantization and model compression. |\\n| **Explainable AI (XAI)** | Growing emphasis on transparent, interpretable models for regulated industries. |\\n| **Responsible AI** | Systematic frameworks for fairness, privacy (differential privacy), and robustness against adversarial attacks. |\\n| **AutoML** | Automated pipeline building (feature engineering, model selection, hyper‑parameter tuning) to democratize ML. |\\n| **Hybrid AI** | Combining symbolic reasoning with neural networks for better reasoning and data efficiency. |\\n\\n---\\n\\n## Quick Glossary\\n\\n- **Epoch** – One full pass through the training dataset.\\n- **Batch / Mini‑batch** – Subset of training samples used to compute a gradient step.\\n- **Learning Rate** – Step size for updating model parameters during optimization.\\n- **Embedding** – Dense vector representation of high‑dimensional categorical or textual data.\\n- **Dropout** – Randomly zeroes out neurons during training to reduce overfitting.\\n- **Transfer Learning** – Re‑using a model trained on one task for another, often with fine‑tuning.\\n- **Ensemble** – Combining multiple models (e.g., bagging, boosting) to improve performance.\\n\\n---\\n\\n### TL;DR\\n\\nMachine learning lets computers learn patterns from data instead of following hand‑written rules. It works by training models—mathematical functions—on labeled (supervised) or unlabeled (unsupervised) data, optimizing a loss function, and evaluating how well the model generalizes to new inputs. The field includes a rich toolbox of algorithms (linear models, trees, neural nets, clustering, RL, etc.), a standard workflow (collect → clean → feature → train → evaluate → deploy), and countless applications across industry and research. Staying mindful of data quality, model interpretability, and ethical considerations is as important as mastering the algorithms themselves.' additional_kwargs={'reasoning_content': 'The user asks \"explain about machine learning?\" Simple request. Provide an overview: definition, types (supervised, unsupervised, reinforcement), key concepts (features, labels, models, training, loss, overfitting, regularization), algorithms (linear regression, decision trees, SVM, neural networks), applications, pipeline, challenges, future. Should be concise but thorough. No disallowed content. Provide friendly explanation.'} response_metadata={'token_usage': {'completion_tokens': 2266, 'prompt_tokens': 77, 'total_tokens': 2343, 'completion_time': 4.849267562, 'prompt_time': 0.003161482, 'queue_time': 5.077415806, 'total_time': 4.852429044}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_5c888d599f', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--8cedfe83-a88f-4e7a-a609-8be5e34ba5d4-0' usage_metadata={'input_tokens': 77, 'output_tokens': 2266, 'total_tokens': 2343}\n",
      "Assistant ## What Is Machine Learning?\n",
      "\n",
      "Machine Learning (ML) is a branch of artificial intelligence that gives computers the ability to **learn from data** and improve their performance on a task without being explicitly programmed for every possible scenario. Instead of writing step‑by‑step rules, we feed the system examples, let it discover patterns, and then use those patterns to make predictions or decisions on new, unseen data.\n",
      "\n",
      "---\n",
      "\n",
      "## Core Concepts\n",
      "\n",
      "| Concept | What It Means | Example |\n",
      "|---------|----------------|---------|\n",
      "| **Data** | The raw information (numbers, text, images, etc.) that the model learns from. | A table of house prices, a collection of photos, a log of user clicks. |\n",
      "| **Features** | Individual measurable properties extracted from the data. | Square footage, number of bedrooms, pixel intensities. |\n",
      "| **Label / Target** | The outcome we want the model to predict (only in supervised learning). | Sale price, “cat vs. dog”, next‑day stock return. |\n",
      "| **Model** | A mathematical function (or a network of functions) that maps features → predictions. | Linear regression line, decision tree, neural network. |\n",
      "| **Training** | The process of adjusting the model’s parameters so it fits the training data. | Gradient descent updating weights in a neural net. |\n",
      "| **Loss / Cost Function** | A metric that quantifies how far the model’s predictions are from the true labels. | Mean Squared Error, Cross‑Entropy. |\n",
      "| **Optimization** | An algorithm (e.g., gradient descent) that minimizes the loss by tweaking parameters. |\n",
      "| **Evaluation** | Measuring model performance on data it hasn’t seen during training (validation / test sets). | Accuracy, F1‑score, ROC‑AUC, RMSE. |\n",
      "| **Generalization** | The ability of a model to perform well on new, unseen data. |\n",
      "| **Overfitting / Underfitting** | Overfitting: model memorizes training data, performs poorly on new data. <br>Underfitting: model is too simple to capture underlying patterns. |\n",
      "| **Regularization** | Techniques (L1/L2 penalties, dropout, early stopping) that discourage overfitting. |\n",
      "\n",
      "---\n",
      "\n",
      "## Main Types of Machine Learning\n",
      "\n",
      "| Type | How It Works | Typical Tasks |\n",
      "|------|--------------|---------------|\n",
      "| **Supervised Learning** | Learns a mapping from **features → labels** using a labeled dataset. | Classification (spam vs. not spam), Regression (predict house price). |\n",
      "| **Unsupervised Learning** | Finds structure in **unlabeled** data—no explicit target. | Clustering (customer segmentation), Dimensionality reduction (PCA, t‑SNE). |\n",
      "| **Semi‑Supervised Learning** | Uses a small amount of labeled data plus a large amount of unlabeled data. | Improving image classifiers when labeling is expensive. |\n",
      "| **Reinforcement Learning (RL)** | An **agent** interacts with an environment, receives rewards, and learns a policy to maximize cumulative reward. | Game playing (AlphaGo), robotics, recommendation systems. |\n",
      "| **Self‑Supervised Learning** | Creates its own supervision signal from raw data (e.g., predicting masked words in a sentence). | Large‑scale language models (GPT, BERT), vision transformers. |\n",
      "\n",
      "---\n",
      "\n",
      "## Common Algorithms (A Quick Tour)\n",
      "\n",
      "| Category | Algorithm | When It’s Useful |\n",
      "|----------|-----------|------------------|\n",
      "| **Linear Models** | Linear Regression, Logistic Regression | Interpretable baselines; small‑to‑medium data. |\n",
      "| **Tree‑Based** | Decision Trees, Random Forests, Gradient Boosting (XGBoost, LightGBM, CatBoost) | Tabular data, handling mixed feature types, strong performance out‑of‑the‑box. |\n",
      "| **Support Vector Machines (SVM)** | Linear / kernel SVM | High‑dimensional spaces, limited data, clear margin. |\n",
      "| **Nearest Neighbors** | k‑Nearest Neighbors (k‑NN) | Simple, non‑parametric; works well for low‑dimensional data. |\n",
      "| **Neural Networks** | Feed‑forward (MLP), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Transformers | Image, speech, text, sequential data; large datasets & GPUs. |\n",
      "| **Clustering** | k‑Means, DBSCAN, Hierarchical Clustering | Discover groups in unlabeled data. |\n",
      "| **Dimensionality Reduction** | Principal Component Analysis (PCA), t‑SNE, UMAP | Visualisation, noise reduction, speed‑up. |\n",
      "| **Reinforcement Learning** | Q‑Learning, Deep Q‑Network (DQN), Proximal Policy Optimization (PPO) | Game AI, robotics, control problems. |\n",
      "\n",
      "---\n",
      "\n",
      "## Typical Machine‑Learning Workflow\n",
      "\n",
      "1. **Define the Problem** – classification, regression, clustering, etc.\n",
      "2. **Collect & Explore Data** – gather raw data, visualize, compute statistics.\n",
      "3. **Preprocess** – clean missing values, encode categorical variables, scale/normalize, augment (for images/audio).\n",
      "4. **Feature Engineering** – create informative features, dimensionality reduction, feature selection.\n",
      "5. **Split Data** – training set (≈70‑80 %), validation set (≈10‑15 %), test set (≈10‑15 %).\n",
      "6. **Choose a Model** – start with simple baseline, then try more complex algorithms.\n",
      "7. **Train** – feed the training data, tune hyper‑parameters (learning rate, depth, regularization strength, etc.).\n",
      "8. **Validate** – monitor performance on validation set; use techniques like cross‑validation.\n",
      "9. **Test** – final unbiased evaluation on the hold‑out test set.\n",
      "10. **Deploy** – serve the model via API, embed in an app, or schedule batch predictions.\n",
      "11. **Monitor & Maintain** – track drift, retrain with new data, update as needed.\n",
      "\n",
      "---\n",
      "\n",
      "## Practical Tips for Success\n",
      "\n",
      "| Tip | Why It Helps |\n",
      "|-----|--------------|\n",
      "| **Start Simple** | Baseline models (logistic regression, random forest) give a performance reference and are easy to interpret. |\n",
      "| **Feature Importance** | Understanding which features drive predictions aids debugging and stakeholder trust. |\n",
      "| **Cross‑Validation** | Reduces variance in performance estimates, especially when data is limited. |\n",
      "| **Hyper‑Parameter Search** | Grid search, random search, or Bayesian optimization can uncover better configurations. |\n",
      "| **Regularization & Early Stopping** | Prevents overfitting, especially for high‑capacity models (deep nets). |\n",
      "| **Data Quality > Model Complexity** | Clean, representative data often yields bigger gains than a fancier algorithm. |\n",
      "| **Explainability** | Tools like SHAP, LIME, or feature importance plots help explain decisions to non‑technical audiences. |\n",
      "| **Ethics & Bias Checks** | Inspect training data and model outputs for unfair bias; consider fairness metrics. |\n",
      "| **Scalability** | For large datasets, use distributed training frameworks (Spark MLlib, TensorFlow Distributed, PyTorch DDP). |\n",
      "| **Version Control** | Keep track of data, code, and model versions (e.g., DVC, MLflow). |\n",
      "\n",
      "---\n",
      "\n",
      "## Real‑World Applications\n",
      "\n",
      "| Domain | Example Use Cases |\n",
      "|--------|-------------------|\n",
      "| **Healthcare** | Disease diagnosis from medical images, drug discovery, patient risk scoring. |\n",
      "| **Finance** | Credit scoring, fraud detection, algorithmic trading, portfolio optimization. |\n",
      "| **Retail & E‑Commerce** | Product recommendation, demand forecasting, dynamic pricing. |\n",
      "| **Transportation** | Route optimization, autonomous driving perception, traffic prediction. |\n",
      "| **Natural Language Processing** | Sentiment analysis, chatbots, machine translation, summarization. |\n",
      "| **Computer Vision** | Face recognition, object detection, video analytics, quality inspection. |\n",
      "| **Manufacturing** | Predictive maintenance, defect detection, process optimization. |\n",
      "| **Entertainment** | Content recommendation, music generation, video game AI. |\n",
      "\n",
      "---\n",
      "\n",
      "## Current Trends & Future Directions\n",
      "\n",
      "| Trend | What It Means |\n",
      "|-------|----------------|\n",
      "| **Foundation Models** | Huge, pre‑trained models (e.g., GPT‑4, CLIP, DALL·E) that can be fine‑tuned for many downstream tasks. |\n",
      "| **Self‑Supervised Learning** | Learning useful representations without labels, reducing reliance on expensive annotation. |\n",
      "| **Edge & TinyML** | Deploying ML models on low‑power devices (IoT, smartphones) with quantization and model compression. |\n",
      "| **Explainable AI (XAI)** | Growing emphasis on transparent, interpretable models for regulated industries. |\n",
      "| **Responsible AI** | Systematic frameworks for fairness, privacy (differential privacy), and robustness against adversarial attacks. |\n",
      "| **AutoML** | Automated pipeline building (feature engineering, model selection, hyper‑parameter tuning) to democratize ML. |\n",
      "| **Hybrid AI** | Combining symbolic reasoning with neural networks for better reasoning and data efficiency. |\n",
      "\n",
      "---\n",
      "\n",
      "## Quick Glossary\n",
      "\n",
      "- **Epoch** – One full pass through the training dataset.\n",
      "- **Batch / Mini‑batch** – Subset of training samples used to compute a gradient step.\n",
      "- **Learning Rate** – Step size for updating model parameters during optimization.\n",
      "- **Embedding** – Dense vector representation of high‑dimensional categorical or textual data.\n",
      "- **Dropout** – Randomly zeroes out neurons during training to reduce overfitting.\n",
      "- **Transfer Learning** – Re‑using a model trained on one task for another, often with fine‑tuning.\n",
      "- **Ensemble** – Combining multiple models (e.g., bagging, boosting) to improve performance.\n",
      "\n",
      "---\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "Machine learning lets computers learn patterns from data instead of following hand‑written rules. It works by training models—mathematical functions—on labeled (supervised) or unlabeled (unsupervised) data, optimizing a loss function, and evaluating how well the model generalizes to new inputs. The field includes a rich toolbox of algorithms (linear models, trees, neural nets, clustering, RL, etc.), a standard workflow (collect → clean → feature → train → evaluate → deploy), and countless applications across industry and research. Staying mindful of data quality, model interpretability, and ethical considerations is as important as mastering the algorithms themselves.\n",
      "Bye...\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"quit\",\"q\"]:\n",
    "        print(\"Bye...\")\n",
    "        break\n",
    "    for event in graph.stream({'messages':('user',user_input)}):\n",
    "        print(event.values())\n",
    "        for value in event.values():\n",
    "            print(value['messages']) \n",
    "            print(\"Assistant\",value['messages'].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f1c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
